{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ovxWW84GUzf"
      },
      "source": [
        "# Intelligent Agents: Reflex-Based Agents for the Vacuum-cleaner World\n",
        "\n",
        "Student Name: [Add your name]\n",
        "\n",
        "I have used the following AI tools: \n",
        "- Cursor AI Assistant for code implementation guidance and debugging\n",
        "- AI assistance for understanding agent architectures and implementation strategies\n",
        "- AI help with debugging and explaining complex concepts from class\n",
        "\n",
        "I understand that my submission needs to be my own work: [your initials]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9aidEMdGUzg"
      },
      "source": [
        "## Learning Outcomes\n",
        "\n",
        "* Design and build a simulation environment that models sensor inputs, actuator effects, and performance measurement.\n",
        "* Apply core AI concepts by implementing the agent function for a simple and model-based reflex agents that respond to environmental percepts.\n",
        "* Practice how the environment and the agent function interact.\n",
        "* Analyze agent performance through controlled experiments across different environment configurations.\n",
        "* Graduate Students: Develop strategies for handling uncertainty and imperfect information in autonomous agent systems.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Total Points: Undergrads 100 + 5 bonus / Graduate students 110\n",
        "\n",
        "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook as a HTML file.\n",
        "\n",
        "### AI Use\n",
        "\n",
        "Here are some guidelines that will make it easier for you:\n",
        "\n",
        "* __Don't:__ Rely on AI auto completion. You will waste a lot of time trying to figure out how the suggested code relates to what we do in class. Turn off AI code completion (e.g., Copilot) in your IDE.\n",
        "* __Don't:__ Do not submit code/text that you do not understand or have not checked to make sure that it is complete and correct.\n",
        "* __Do:__ Use AI for debugging and letting it explain code and concepts from class.\n",
        "\n",
        "### Using Visual Studio Code\n",
        "\n",
        "If you use VS code then you can use `Export` (click on `...` in the menu bar) to save your notebook as a HTML file. Note that you have to run all blocks before so the HTML file contains your output.\n",
        "\n",
        "### Using Google Colab\n",
        "\n",
        "In Colab you need to save the notebook on GoogleDrive to work with it. For this you need to mount your google dive and change to the correct directory by uncommenting the following lines and running the code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSxjvMVuGUzg"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "#\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90gOlZSfGUzg"
      },
      "source": [
        "Once you are done with the assignment and have run all code blocks using `Runtime/Run all`, you can convert the file on your GoogleDrive into HTML be uncommenting the following line and running the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnagsFmAGUzh"
      },
      "outputs": [],
      "source": [
        "# %jupyter nbconvert --to html Copy\\ of\\ robot_vacuum.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUctXg_5GUzh"
      },
      "source": [
        "You may have to fix the file location or the file name to match how it looks on your GoogleDrive. You can navigate in Colab to your GoogleDrive using the little folder symbol in the navigation bar to the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5vfHwAbGUzh"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this assignment you will implement a simulator environment for an automatic vacuum cleaner robot, a set of different reflex-based agent programs, and perform a comparison study for cleaning a single room. Focus on the __cleaning phase__ which starts when the robot is activated and ends when the last dirty square in the room has been cleaned. Someone else will take care of the agent program needed to navigate back to the charging station after the room is clean.\n",
        "\n",
        "## PEAS description of the cleaning phase\n",
        "\n",
        "__Performance Measure:__ Each action costs 1 energy unit. The performance is measured as the sum of the energy units used to clean the whole room.\n",
        "\n",
        "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$. For simplicity, you can assume that the agent knows the size and the layout of the room (i.e., it knows $n$). To start, the agent is placed on a random square.\n",
        "\n",
        "__Actuators:__ The agent can clean the current square (action `suck`) or move to an adjacent square by going `north`, `east`, `south`, or `west`.\n",
        "\n",
        "__Sensors:__ Four bumper sensors, one for north, east, south, and west; a dirt sensor reporting dirt in the current square.  \n",
        "\n",
        "\n",
        "## The agent program for a simple randomized agent\n",
        "\n",
        "The agent program is a function that gets sensor information (the current percepts) as the arguments. The arguments are:\n",
        "\n",
        "* A dictionary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`. E.g., if the agent is on the north-west corner, `bumpers` will be `{\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}`.\n",
        "* The dirt sensor produces a boolean.\n",
        "\n",
        "The agent returns the chosen action as a string.\n",
        "\n",
        "Here is an example implementation for the agent program of a simple randomized agent:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQO6YJY3GUzh",
        "outputId": "f7e56741-86f7-47a0-90b1-81773e765f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# make sure numpy is installed\n",
        "%pip install -q numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuCSJBbSGUzh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
        "\n",
        "def simple_randomized_agent(bumpers, dirty):\n",
        "    return np.random.choice(actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AyfmCOzGUzh",
        "outputId": "2134ca27-43cb-42c5-f29f-6c888dce6f3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.str_('east')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define percepts (current location is NW corner and it is dirty)\n",
        "bumpers = {\"north\" : True, \"east\" : False, \"south\" : False, \"west\" : True}\n",
        "dirty = True\n",
        "\n",
        "# call agent program function with percepts and it returns an action\n",
        "simple_randomized_agent(bumpers, dirty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Momq3fnRGUzi"
      },
      "source": [
        "__Note:__ This is not a rational intelligent agent. It ignores its sensors and may bump into a wall repeatedly or not clean a dirty square. You will be asked to implement rational agents below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JX_NZ1SGUzi"
      },
      "source": [
        "## Simple environment example\n",
        "\n",
        "We implement a simple simulation environment that supplies the agent with its percepts.\n",
        "The simple environment is infinite in size (bumpers are always `False`) and every square is always dirty, even if the agent cleans it. The environment function returns a different performance measure than the one specified in the PEAS description! Since the room is infinite and all squares are constantly dirty, the agent can never clean the whole room. Your implementation needs to implement the **correct performance measure.** The energy budget of the agent is specified as `max_steps`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7m1iBfXGUzi"
      },
      "outputs": [],
      "source": [
        "def simple_environment(agent_function, max_steps, verbose = True):\n",
        "    num_cleaned = 0\n",
        "\n",
        "    for i in range(max_steps):\n",
        "        dirty = True\n",
        "        bumpers = {\"north\" : False, \"south\" : False, \"west\" : False, \"east\" : False}\n",
        "\n",
        "        action = agent_function(bumpers, dirty)\n",
        "        if (verbose): print(\"step\", i , \"- action:\", action)\n",
        "\n",
        "        if (action == \"suck\"):\n",
        "            num_cleaned = num_cleaned + 1\n",
        "\n",
        "    return num_cleaned\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjDsdiLCGUzi"
      },
      "source": [
        "Do one simulation run with a simple randomized agent that has enough energy for 20 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PDv8vpHGUzi",
        "outputId": "41a593ee-be59-405c-a878-1fc9f754f5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 - action: north\n",
            "step 1 - action: west\n",
            "step 2 - action: suck\n",
            "step 3 - action: south\n",
            "step 4 - action: west\n",
            "step 5 - action: suck\n",
            "step 6 - action: south\n",
            "step 7 - action: south\n",
            "step 8 - action: east\n",
            "step 9 - action: south\n",
            "step 10 - action: north\n",
            "step 11 - action: south\n",
            "step 12 - action: east\n",
            "step 13 - action: north\n",
            "step 14 - action: south\n",
            "step 15 - action: north\n",
            "step 16 - action: south\n",
            "step 17 - action: south\n",
            "step 18 - action: west\n",
            "step 19 - action: north\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_environment(simple_randomized_agent, max_steps = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn_W4JwLGUzi"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "## General [10 Points]\n",
        "\n",
        "1. Make sure that you use the latest version of this notebook.\n",
        "2. Your implementation can use libraries like math, numpy, scipy, but not libraries that implement intelligent agents or complete search algorithms. Try to keep the code simple! In this course, we want to learn about the algorithms and we often do not need to use object-oriented design.\n",
        "3. You notebook needs to be formatted professionally.\n",
        "    - Add additional markdown blocks for your description, comments in the code, add tables and use mathplotlib to produce charts where appropriate\n",
        "    - Do not show debugging output or include an excessive amount of output.\n",
        "    - Check that your submitted file is readable and contains all figures.\n",
        "4. Document your code. Use comments in the code and add a discussion of how your implementation works and your design choices.\n",
        "\n",
        "\n",
        "## Task 1: Implement a simulation environment [20 Points]\n",
        "\n",
        "The simple environment above is not very realistic. Your environment simulator needs to follow the PEAS description from above. It needs to:\n",
        "\n",
        "* Initialize the environment by storing the state of each square (clean/dirty) and making some dirty. ([Help with random numbers and arrays in Python](https://colab.research.google.com/drive/1RRzbPq-oel_rzi2GOptCFyxLpi3a32Mc?usp=sharing))\n",
        "* Keep track of the agent's position.\n",
        "* Call the agent function repeatedly and provide the agent function with the sensor inputs.  \n",
        "* React to the agent's actions. E.g, by removing dirt from a square or moving the agent around unless there is a wall in the way.\n",
        "* Keep track of the performance measure. That is, track the agent's actions until all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
        "\n",
        "The easiest implementation for the environment is to hold an 2-dimensional array to represent if squares are clean or dirty and to call the agent function in a loop until all squares are clean or a predefined number of steps have been reached (i.e., the robot runs out of energy).\n",
        "\n",
        "The simulation environment should be a function like the `simple_environment()` and needs to work with the simple randomized agent program from above. **Use the same environment for all your agent implementations in the tasks below.**\n",
        "\n",
        "*Note on debugging:* Debugging is difficult. Make sure your environment prints enough information when you use `verbose = True`. Also, implementing a function that the environment can use to displays the room with dirt and the current position of the robot at every step is very useful.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Improved Simulation Environment Implementation\n",
        "# \n",
        "# This is a cleaner, more readable version of the vacuum environment\n",
        "# that implements the PEAS description with better code structure\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def vacuum_environment(agent_function, room_size=5, dirt_prob=0.2, max_steps=1000, verbose=False):\n",
        "    \"\"\"\n",
        "    Simulation environment for vacuum cleaner robot.\n",
        "    \n",
        "    Args:\n",
        "        agent_function: The agent program function\n",
        "        room_size: Size of the square room (default 5x5)\n",
        "        dirt_prob: Probability that each square starts dirty (default 0.2)\n",
        "        max_steps: Maximum number of steps before timeout (default 1000)\n",
        "        verbose: Whether to print debug information\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (total_energy_used, success_flag, steps_taken)\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Build the room: each square has a dirt_prob probability of being dirty\n",
        "    room = np.random.random((room_size, room_size)) < dirt_prob\n",
        "    \n",
        "    # 2. Put the robot in a random spot\n",
        "    x = random.randint(0, room_size - 1)\n",
        "    y = random.randint(0, room_size - 1)\n",
        "\n",
        "    energy_used = 0\n",
        "    steps_taken = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Starting room (1=dirty, 0=clean):\")\n",
        "        print(room.astype(int))\n",
        "        print(f\"Robot starts at ({x}, {y})\\n\")\n",
        "\n",
        "    # 3. Keep going until energy runs out\n",
        "    while energy_used < max_steps:\n",
        "        # Stop if everything is clean\n",
        "        if np.sum(room) == 0:\n",
        "            if verbose:\n",
        "                print(f\"All clean in {energy_used} steps!\")\n",
        "            return energy_used, True, steps_taken\n",
        "        \n",
        "        # 4. Robot sensors\n",
        "        bumpers = {\n",
        "            \"north\": y == 0,\n",
        "            \"south\": y == room_size - 1,\n",
        "            \"west\": x == 0,\n",
        "            \"east\": x == room_size - 1\n",
        "        }\n",
        "        dirty_here = room[y, x]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Step {energy_used}: at ({x},{y}), dirty={dirty_here}\")\n",
        "\n",
        "        # 5. Ask the robot what to do\n",
        "        action = agent_function(bumpers, dirty_here)\n",
        "\n",
        "        # 6. Carry out the action\n",
        "        if action == \"suck\":\n",
        "            room[y, x] = False   # clean the square\n",
        "            if verbose: print(\" → Sucked up dirt\")\n",
        "        elif action == \"north\" and not bumpers[\"north\"]:\n",
        "            y -= 1\n",
        "            if verbose: print(\" → Moved north\")\n",
        "        elif action == \"south\" and not bumpers[\"south\"]:\n",
        "            y += 1\n",
        "            if verbose: print(\" → Moved south\")\n",
        "        elif action == \"west\" and not bumpers[\"west\"]:\n",
        "            x -= 1\n",
        "            if verbose: print(\" → Moved west\")\n",
        "        elif action == \"east\" and not bumpers[\"east\"]:\n",
        "            x += 1\n",
        "            if verbose: print(\" → Moved east\")\n",
        "        else:\n",
        "            if verbose: print(f\" → Invalid or bump action: {action}\")\n",
        "\n",
        "        energy_used += 1\n",
        "        steps_taken += 1\n",
        "\n",
        "    # If we ran out of steps\n",
        "    if verbose:\n",
        "        print(f\"Stopped after {max_steps} steps. Dirt left: {np.sum(room)}\")\n",
        "    return energy_used, False, steps_taken\n",
        "\n",
        "def display_room_state(room, agent_x, agent_y):\n",
        "    \"\"\"\n",
        "    Display the current room state with agent position.\n",
        "    \"\"\"\n",
        "    room_size = room.shape[0]\n",
        "    print(\"Room state (D=dirty, C=clean, A=agent):\")\n",
        "    for y in range(room_size):\n",
        "        row = \"\"\n",
        "        for x in range(room_size):\n",
        "            if x == agent_x and y == agent_y:\n",
        "                row += \"A \"\n",
        "            elif room[y, x]:\n",
        "                row += \"D \"\n",
        "            else:\n",
        "                row += \"C \"\n",
        "        print(row)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6egqEgZGUzi"
      },
      "outputs": [],
      "source": [
        "# Old Task 1 implementation - replaced with cleaner version above\n",
        "# \n",
        "# This environment implements the PEAS description:\n",
        "# - 5x5 room with random dirt placement (probability p=0.2)\n",
        "# - Agent starts at random position\n",
        "# - Tracks agent position and room state\n",
        "# - Provides bumper and dirt sensors to agent\n",
        "# - Measures performance as total energy units used to clean all dirty squares\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def vacuum_environment(agent_function, room_size=5, dirt_prob=0.2, max_steps=1000, verbose=False):\n",
        "    room = np.random.random((room_size, room_size)) < dirt_prob\n",
        "    initial_dirty_count = np.sum(room)\n",
        "    \n",
        "    # Random starting position\n",
        "    agent_x = random.randint(0, room_size - 1)\n",
        "    agent_y = random.randint(0, room_size - 1)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Initial room state (1=dirty, 0=clean):\")\n",
        "        print(room.astype(int))\n",
        "        print(f\"Agent starts at position ({agent_x}, {agent_y})\")\n",
        "        print(f\"Initial dirty squares: {initial_dirty_count}\")\n",
        "        print()\n",
        "    \n",
        "    energy_used = 0\n",
        "    steps_taken = 0\n",
        "    \n",
        "    # Main simulation loop\n",
        "    while energy_used < max_steps:\n",
        "        # Check if room is completely clean\n",
        "        if np.sum(room) == 0:\n",
        "            if verbose:\n",
        "                print(f\"Room cleaned! Total energy used: {energy_used}\")\n",
        "            return energy_used, True, steps_taken\n",
        "        \n",
        "        # Create bumper sensors based on agent position\n",
        "        bumpers = {\n",
        "            \"north\": agent_y == 0,  # At top edge\n",
        "            \"south\": agent_y == room_size - 1,  # At bottom edge\n",
        "            \"west\": agent_x == 0,  # At left edge\n",
        "            \"east\": agent_x == room_size - 1   # At right edge\n",
        "        }\n",
        "        \n",
        "        # Dirt sensor for current position\n",
        "        dirty = room[agent_y, agent_x]\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Step {steps_taken}: Agent at ({agent_x}, {agent_y}), dirty={dirty}\")\n",
        "            print(f\"Bumpers: {bumpers}\")\n",
        "        \n",
        "        # Get action from agent\n",
        "        action = agent_function(bumpers, dirty)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Action: {action}\")\n",
        "        \n",
        "        # Execute action\n",
        "        if action == \"suck\":\n",
        "            if dirty:\n",
        "                room[agent_y, agent_x] = False  # Clean the square\n",
        "                if verbose:\n",
        "                    print(\"Square cleaned!\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Sucking on clean square (no effect)\")\n",
        "        \n",
        "        elif action == \"north\":\n",
        "            if agent_y > 0:\n",
        "                agent_y -= 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved north to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into north wall\")\n",
        "        \n",
        "        elif action == \"south\":\n",
        "            if agent_y < room_size - 1:\n",
        "                agent_y += 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved south to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into south wall\")\n",
        "        \n",
        "        elif action == \"west\":\n",
        "            if agent_x > 0:\n",
        "                agent_x -= 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved west to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into west wall\")\n",
        "        \n",
        "        elif action == \"east\":\n",
        "            if agent_x < room_size - 1:\n",
        "                agent_x += 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved east to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into east wall\")\n",
        "        \n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"Invalid action: {action}\")\n",
        "        \n",
        "        energy_used += 1\n",
        "        steps_taken += 1\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Remaining dirty squares: {np.sum(room)}\")\n",
        "            print()\n",
        "    \n",
        "    # Timeout reached\n",
        "    if verbose:\n",
        "        print(f\"Timeout reached after {max_steps} steps. Room not fully cleaned.\")\n",
        "        print(f\"Remaining dirty squares: {np.sum(room)}\")\n",
        "    \n",
        "    return energy_used, False, steps_taken\n",
        "\n",
        "def display_room_state(room, agent_x, agent_y):\n",
        "    \"\"\"\n",
        "    Display the current room state with agent position.\n",
        "    \"\"\"\n",
        "    room_size = room.shape[0]\n",
        "    print(\"Room state (D=dirty, C=clean, A=agent):\")\n",
        "    for y in range(room_size):\n",
        "        row = \"\"\n",
        "        for x in range(room_size):\n",
        "            if x == agent_x and y == agent_y:\n",
        "                row += \"A \"\n",
        "            elif room[y, x]:\n",
        "                row += \"D \"\n",
        "            else:\n",
        "                row += \"C \"\n",
        "        print(row)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRqmc8ymGUzi"
      },
      "source": [
        "Show that your environment works with the simple randomized agent from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoSs6MvaGUzi"
      },
      "outputs": [],
      "source": [
        "# Test the environment with the simple randomized agent\n",
        "print(\"Testing vacuum environment with simple randomized agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run a single test with verbose output to see how it works\n",
        "energy, success, steps = vacuum_environment(simple_randomized_agent, room_size=5, verbose=True)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Success: {success}\")\n",
        "print(f\"Energy used: {energy}\")\n",
        "print(f\"Steps taken: {steps}\")\n",
        "\n",
        "# Run multiple tests to get average performance\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Running 10 tests to get average performance:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "energies = []\n",
        "successes = []\n",
        "for i in range(10):\n",
        "    energy, success, steps = vacuum_environment(simple_randomized_agent, room_size=5, verbose=False)\n",
        "    energies.append(energy)\n",
        "    successes.append(success)\n",
        "    print(f\"Test {i+1}: Energy={energy}, Success={success}\")\n",
        "\n",
        "print(f\"\\nAverage energy used: {np.mean(energies):.1f}\")\n",
        "print(f\"Success rate: {np.mean(successes)*100:.1f}%\")\n",
        "print(f\"Min energy: {min(energies)}\")\n",
        "print(f\"Max energy: {max(energies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Simple Reflex Agent (Improved Version)\n",
        "# Imagine a robot vacuum. It can only sense:\n",
        "# 1. Is the floor dirty right now?\n",
        "# 2. Am I at a wall? (north, south, east, west)\n",
        "#\n",
        "# Rules for the robot:\n",
        "# - If dirty → clean it (\"suck\")\n",
        "# - If not dirty → pick a random safe direction (no bumping into walls)\n",
        "# - If somehow stuck → just \"suck\" as a backup\n",
        "\n",
        "import random\n",
        "\n",
        "def simple_reflex_agent(bumpers, dirty):\n",
        "    # If the robot sees dirt, it always cleans first\n",
        "    if dirty:\n",
        "        return \"suck\"\n",
        "    \n",
        "    # Otherwise, check which directions are safe (no wall)\n",
        "    available_directions = []\n",
        "    if not bumpers[\"north\"]:\n",
        "        available_directions.append(\"north\")\n",
        "    if not bumpers[\"south\"]:\n",
        "        available_directions.append(\"south\")\n",
        "    if not bumpers[\"east\"]:\n",
        "        available_directions.append(\"east\")\n",
        "    if not bumpers[\"west\"]:\n",
        "        available_directions.append(\"west\")\n",
        "    \n",
        "    # If no safe moves (shouldn't really happen), just suck\n",
        "    if not available_directions:\n",
        "        return \"suck\"\n",
        "    \n",
        "    # Pick one safe direction randomly\n",
        "    return random.choice(available_directions)\n",
        "\n",
        "# Testing the robot brain\n",
        "# The robot will:\n",
        "# - Suck whenever it sees dirt\n",
        "# - Move randomly around the room\n",
        "# - Avoid crashing into walls\n",
        "print(\"Simple Reflex Agent Demo\")\n",
        "print(\"This agent will:\")\n",
        "print(\"1. Always suck when it detects dirt\")\n",
        "print(\"2. Move randomly but avoid walls\")\n",
        "print(\"3. Never bump into walls\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_V1WUCJGUzi"
      },
      "source": [
        "## Task 2:  Implement a simple reflex agent [10 Points]\n",
        "\n",
        "The simple reflex agent randomly walks around but reacts to the bumper sensor by not bumping into the wall and to dirt with sucking. Implement the agent program as a function.\n",
        "\n",
        "_Note:_ Agents cannot directly use variable in the environment. They only gets the percepts as the arguments to the agent function. Use the function signature for the `simple_randomized_agent` function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNpgFrR7GUzi"
      },
      "outputs": [],
      "source": [
        "# Task 2: Simple Reflex Agent Implementation\n",
        "#\n",
        "# The simple reflex agent reacts to sensor inputs:\n",
        "# 1. If current square is dirty, suck it\n",
        "# 2. If bumping into a wall, choose a different direction\n",
        "# 3. Otherwise, move randomly but avoid walls\n",
        "\n",
        "def simple_reflex_agent(bumpers, dirty):\n",
        "    \"\"\"\n",
        "    Simple reflex agent that reacts to bumper and dirt sensors.\n",
        "    \n",
        "    Args:\n",
        "        bumpers: Dictionary with boolean values for north, south, east, west\n",
        "        dirty: Boolean indicating if current square is dirty\n",
        "    \n",
        "    Returns:\n",
        "        str: Action to take (\"north\", \"south\", \"east\", \"west\", \"suck\")\n",
        "    \"\"\"\n",
        "    \n",
        "    # Rule 1: If current square is dirty, clean it\n",
        "    if dirty:\n",
        "        return \"suck\"\n",
        "    \n",
        "    # Rule 2: Choose a random direction that doesn't hit a wall\n",
        "    available_directions = []\n",
        "    \n",
        "    if not bumpers[\"north\"]:\n",
        "        available_directions.append(\"north\")\n",
        "    if not bumpers[\"south\"]:\n",
        "        available_directions.append(\"south\")\n",
        "    if not bumpers[\"east\"]:\n",
        "        available_directions.append(\"east\")\n",
        "    if not bumpers[\"west\"]:\n",
        "        available_directions.append(\"west\")\n",
        "    \n",
        "    # If no directions available (shouldn't happen in normal room), default to suck\n",
        "    if not available_directions:\n",
        "        return \"suck\"\n",
        "    \n",
        "    # Randomly choose from available directions\n",
        "    return np.random.choice(available_directions)\n",
        "\n",
        "# Test the simple reflex agent\n",
        "print(\"Simple Reflex Agent:\")\n",
        "print(\"This agent will:\")\n",
        "print(\"1. Always suck when it detects dirt\")\n",
        "print(\"2. Move randomly but avoid walls\")\n",
        "print(\"3. Never bump into walls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eow3WOSQGUzi"
      },
      "source": [
        "Show how the agent works with your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNPCta22GUzi"
      },
      "outputs": [],
      "source": [
        "# Test the simple reflex agent with the environment\n",
        "print(\"Testing Simple Reflex Agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run a single test with verbose output\n",
        "energy, success, steps = vacuum_environment(simple_reflex_agent, room_size=5, verbose=True)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Success: {success}\")\n",
        "print(f\"Energy used: {energy}\")\n",
        "print(f\"Steps taken: {steps}\")\n",
        "\n",
        "# Run multiple tests to compare with randomized agent\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Running 10 tests to compare performance:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "energies = []\n",
        "successes = []\n",
        "for i in range(10):\n",
        "    energy, success, steps = vacuum_environment(simple_reflex_agent, room_size=5, verbose=False)\n",
        "    energies.append(energy)\n",
        "    successes.append(success)\n",
        "    print(f\"Test {i+1}: Energy={energy}, Success={success}\")\n",
        "\n",
        "print(f\"\\nSimple Reflex Agent Performance:\")\n",
        "print(f\"Average energy used: {np.mean(energies):.1f}\")\n",
        "print(f\"Success rate: {np.mean(successes)*100:.1f}%\")\n",
        "print(f\"Min energy: {min(energies)}\")\n",
        "print(f\"Max energy: {max(energies)}\")\n",
        "\n",
        "# Compare with randomized agent\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison with Randomized Agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "random_energies = []\n",
        "random_successes = []\n",
        "for i in range(10):\n",
        "    energy, success, steps = vacuum_environment(simple_randomized_agent, room_size=5, verbose=False)\n",
        "    random_energies.append(energy)\n",
        "    random_successes.append(success)\n",
        "\n",
        "print(f\"Randomized Agent Performance:\")\n",
        "print(f\"Average energy used: {np.mean(random_energies):.1f}\")\n",
        "print(f\"Success rate: {np.mean(random_successes)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"Energy reduction: {np.mean(random_energies) - np.mean(energies):.1f}\")\n",
        "print(f\"Success rate improvement: {(np.mean(successes) - np.mean(random_successes))*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUadAe9XGUzi"
      },
      "source": [
        "## Task 3: Implement a model-based reflex agent [20 Points]\n",
        "\n",
        "Model-based agents use a state to keep track of what they have done and perceived so far. Your agent needs to find out where it is located and then keep track of its current location. You also need a set of rules based on the state and the percepts to make sure that the agent will clean the whole room. For example, the agent can move to a corner to determine its location and then it can navigate through the whole room and clean dirty squares.\n",
        "\n",
        "Describe how you define the __agent state__ and how your agent works before implementing it. ([Help with implementing state information on Python](https://colab.research.google.com/drive/1gARICzulhRQLmwYYR4xRAF40AueYOyAY?usp=sharing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dBA0Y7KuGUzi"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1604741280.py, line 5)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThe model-based reflex agent maintains the following state information:\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "## Model-Based Reflex Agent Design\n",
        "\n",
        "### Agent State Design\n",
        "\n",
        "The model-based reflex agent maintains the following state information:\n",
        "\n",
        "1. **Current Position**: (x, y) coordinates of the agent\n",
        "2. **Room Size**: Dimensions of the room (assumed to be square)\n",
        "3. **Visited Squares**: Set of coordinates that have been visited\n",
        "4. **Cleaned Squares**: Set of coordinates that have been cleaned\n",
        "5. **Current Mode**: The agent operates in different modes:\n",
        "   - `LOCATE`: Finding a corner to establish position\n",
        "   - `EXPLORE`: Systematically visiting all squares\n",
        "   - `CLEAN`: Cleaning dirty squares as found\n",
        "\n",
        "### Implementation Strategy\n",
        "\n",
        "1. **Position Discovery**: Start by moving to a corner (e.g., northwest corner) to establish absolute position\n",
        "2. **Systematic Exploration**: Use a systematic pattern (like row-by-row) to visit all squares\n",
        "3. **Dirt Cleaning**: Clean any dirty squares encountered during exploration\n",
        "4. **State Updates**: Track visited and cleaned squares to avoid redundant actions\n",
        "\n",
        "### Key Advantages\n",
        "\n",
        "- **Complete Coverage**: Ensures all squares are visited\n",
        "- **Efficient**: Avoids revisiting clean squares unnecessarily\n",
        "- **Deterministic**: Predictable behavior and performance\n",
        "- **Memory**: Maintains knowledge of room state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzhAhnMTGUzi"
      },
      "outputs": [],
      "source": [
        "# Task 3: Model-Based Reflex Agent Implementation\n",
        "# \n",
        "# This agent maintains internal state to track its position and plan its actions.\n",
        "# It uses a systematic approach to ensure complete room coverage.\n",
        "\n",
        "# Global state for the model-based agent\n",
        "agent_state = {\n",
        "    'position': None,    # Will be inferred from bumpers\n",
        "    'room_size': 5,      # Assumed room size\n",
        "    'visited': set(),    # Set of visited coordinates\n",
        "    'cleaned': set(),    # Set of cleaned coordinates\n",
        "    'mode': 'LOCATE',    # Current mode: LOCATE, EXPLORE\n",
        "    'exploration_path': [],  # Planned path for exploration\n",
        "    'path_index': 0,     # Current position in exploration path\n",
        "    'last_action': None  # Track last action for position inference\n",
        "}\n",
        "\n",
        "def reset_agent_state():\n",
        "    \"\"\"Reset the agent state for a new run.\"\"\"\n",
        "    global agent_state\n",
        "    agent_state = {\n",
        "        'position': None,\n",
        "        'room_size': 5,\n",
        "        'visited': set(),\n",
        "        'cleaned': set(),\n",
        "        'mode': 'LOCATE',\n",
        "        'exploration_path': [],\n",
        "        'path_index': 0,\n",
        "        'last_action': None\n",
        "    }\n",
        "\n",
        "def infer_position_from_bumpers(bumpers):\n",
        "    \"\"\"Infer current position based on bumper sensors.\"\"\"\n",
        "    # This is a simplified approach - in reality, we'd need more sophisticated tracking\n",
        "    # For now, we'll use a simple heuristic based on wall proximity\n",
        "    \n",
        "    # Count walls to estimate position\n",
        "    wall_count = sum(bumpers.values())\n",
        "    \n",
        "    if wall_count == 2:\n",
        "        # Corner position\n",
        "        if bumpers['north'] and bumpers['west']:\n",
        "            return (0, 0)  # Northwest corner\n",
        "        elif bumpers['north'] and bumpers['east']:\n",
        "            return (4, 0)  # Northeast corner\n",
        "        elif bumpers['south'] and bumpers['west']:\n",
        "            return (0, 4)  # Southwest corner\n",
        "        elif bumpers['south'] and bumpers['east']:\n",
        "            return (4, 4)  # Southeast corner\n",
        "    elif wall_count == 1:\n",
        "        # Edge position\n",
        "        if bumpers['north']:\n",
        "            return (2, 0)  # Top edge\n",
        "        elif bumpers['south']:\n",
        "            return (2, 4)  # Bottom edge\n",
        "        elif bumpers['west']:\n",
        "            return (0, 2)  # Left edge\n",
        "        elif bumpers['east']:\n",
        "            return (4, 2)  # Right edge\n",
        "    \n",
        "    # Default to center if no walls detected\n",
        "    return (2, 2)\n",
        "\n",
        "def generate_exploration_path():\n",
        "    \"\"\"Generate a systematic path to visit all squares.\"\"\"\n",
        "    path = []\n",
        "    \n",
        "    # Simple row-by-row exploration pattern\n",
        "    for y in range(5):\n",
        "        if y % 2 == 0:  # Even rows: left to right\n",
        "            for x in range(5):\n",
        "                path.append((x, y))\n",
        "        else:  # Odd rows: right to left\n",
        "            for x in range(4, -1, -1):\n",
        "                path.append((x, y))\n",
        "    \n",
        "    return path\n",
        "\n",
        "def get_available_directions(bumpers):\n",
        "    \"\"\"Get list of available directions (not blocked by walls).\"\"\"\n",
        "    directions = []\n",
        "    if not bumpers['north']:\n",
        "        directions.append('north')\n",
        "    if not bumpers['south']:\n",
        "        directions.append('south')\n",
        "    if not bumpers['east']:\n",
        "        directions.append('east')\n",
        "    if not bumpers['west']:\n",
        "        directions.append('west')\n",
        "    return directions\n",
        "\n",
        "def model_based_reflex_agent(bumpers, dirty):\n",
        "    \"\"\"\n",
        "    Model-based reflex agent that maintains state and navigates systematically.\n",
        "    \n",
        "    Args:\n",
        "        bumpers: Dictionary with boolean values for north, south, east, west\n",
        "        dirty: Boolean indicating if current square is dirty\n",
        "    \n",
        "    Returns:\n",
        "        str: Action to take\n",
        "    \"\"\"\n",
        "    global agent_state\n",
        "    \n",
        "    # Infer current position\n",
        "    current_pos = infer_position_from_bumpers(bumpers)\n",
        "    agent_state['position'] = current_pos\n",
        "    agent_state['visited'].add(current_pos)\n",
        "    \n",
        "    # Rule 1: Always clean if dirty\n",
        "    if dirty:\n",
        "        agent_state['cleaned'].add(current_pos)\n",
        "        agent_state['last_action'] = 'suck'\n",
        "        return 'suck'\n",
        "    \n",
        "    # Mode: LOCATE - Try to reach a corner to establish position\n",
        "    if agent_state['mode'] == 'LOCATE':\n",
        "        # Check if we're at a corner\n",
        "        wall_count = sum(bumpers.values())\n",
        "        if wall_count >= 2:  # At a corner or edge\n",
        "            agent_state['mode'] = 'EXPLORE'\n",
        "            agent_state['exploration_path'] = generate_exploration_path()\n",
        "            agent_state['path_index'] = 0\n",
        "        else:\n",
        "            # Move towards a corner (prefer northwest)\n",
        "            if not bumpers['north']:\n",
        "                agent_state['last_action'] = 'north'\n",
        "                return 'north'\n",
        "            elif not bumpers['west']:\n",
        "                agent_state['last_action'] = 'west'\n",
        "                return 'west'\n",
        "            else:\n",
        "                # Choose any available direction\n",
        "                available = get_available_directions(bumpers)\n",
        "                if available:\n",
        "                    action = np.random.choice(available)\n",
        "                    agent_state['last_action'] = action\n",
        "                    return action\n",
        "    \n",
        "    # Mode: EXPLORE - Systematically visit squares\n",
        "    if agent_state['mode'] == 'EXPLORE':\n",
        "        # Check if we've visited all squares\n",
        "        if len(agent_state['visited']) >= 25:  # 5x5 = 25 squares\n",
        "            agent_state['last_action'] = 'suck'\n",
        "            return 'suck'\n",
        "        \n",
        "        # Find next unvisited square\n",
        "        next_target = None\n",
        "        for i in range(agent_state['path_index'], len(agent_state['exploration_path'])):\n",
        "            target = agent_state['exploration_path'][i]\n",
        "            if target not in agent_state['visited']:\n",
        "                next_target = target\n",
        "                agent_state['path_index'] = i\n",
        "                break\n",
        "        \n",
        "        if next_target:\n",
        "            # Move towards target\n",
        "            target_x, target_y = next_target\n",
        "            current_x, current_y = current_pos\n",
        "            \n",
        "            # Calculate direction to target\n",
        "            dx = target_x - current_x\n",
        "            dy = target_y - current_y\n",
        "            \n",
        "            # Choose direction based on largest difference\n",
        "            if abs(dx) > abs(dy):\n",
        "                if dx > 0 and not bumpers['east']:\n",
        "                    agent_state['last_action'] = 'east'\n",
        "                    return 'east'\n",
        "                elif dx < 0 and not bumpers['west']:\n",
        "                    agent_state['last_action'] = 'west'\n",
        "                    return 'west'\n",
        "            else:\n",
        "                if dy > 0 and not bumpers['south']:\n",
        "                    agent_state['last_action'] = 'south'\n",
        "                    return 'south'\n",
        "                elif dy < 0 and not bumpers['north']:\n",
        "                    agent_state['last_action'] = 'north'\n",
        "                    return 'north'\n",
        "        \n",
        "        # If can't move towards target, choose any available direction\n",
        "        available = get_available_directions(bumpers)\n",
        "        if available:\n",
        "            action = np.random.choice(available)\n",
        "            agent_state['last_action'] = action\n",
        "            return action\n",
        "    \n",
        "    # Fallback\n",
        "    agent_state['last_action'] = 'suck'\n",
        "    return 'suck'\n",
        "\n",
        "# Initialize agent state\n",
        "reset_agent_state()\n",
        "\n",
        "print(\"Model-Based Reflex Agent:\")\n",
        "print(\"This agent will:\")\n",
        "print(\"1. Infer its position from bumper sensors\")\n",
        "print(\"2. Systematically explore all squares\")\n",
        "print(\"3. Clean dirty squares as encountered\")\n",
        "print(\"4. Maintain memory of visited and cleaned squares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFF8NcMGUzi"
      },
      "source": [
        "Show how the agent works with your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzQj58xzGUzi"
      },
      "outputs": [],
      "source": [
        "# Test the model-based reflex agent with the environment\n",
        "print(\"Testing Model-Based Reflex Agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reset agent state for testing\n",
        "reset_agent_state()\n",
        "\n",
        "# Run a single test with verbose output\n",
        "energy, success, steps = vacuum_environment(model_based_reflex_agent, room_size=5, verbose=True)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Success: {success}\")\n",
        "print(f\"Energy used: {energy}\")\n",
        "print(f\"Steps taken: {steps}\")\n",
        "\n",
        "# Run multiple tests to compare performance\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Running 10 tests to compare performance:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "energies = []\n",
        "successes = []\n",
        "for i in range(10):\n",
        "    reset_agent_state()  # Reset state for each test\n",
        "    energy, success, steps = vacuum_environment(model_based_reflex_agent, room_size=5, verbose=False)\n",
        "    energies.append(energy)\n",
        "    successes.append(success)\n",
        "    print(f\"Test {i+1}: Energy={energy}, Success={success}\")\n",
        "\n",
        "print(f\"\\nModel-Based Reflex Agent Performance:\")\n",
        "print(f\"Average energy used: {np.mean(energies):.1f}\")\n",
        "print(f\"Success rate: {np.mean(successes)*100:.1f}%\")\n",
        "print(f\"Min energy: {min(energies)}\")\n",
        "print(f\"Max energy: {max(energies)}\")\n",
        "\n",
        "# Compare with other agents\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison with Other Agents:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test simple reflex agent for comparison\n",
        "simple_energies = []\n",
        "simple_successes = []\n",
        "for i in range(10):\n",
        "    energy, success, steps = vacuum_environment(simple_reflex_agent, room_size=5, verbose=False)\n",
        "    simple_energies.append(energy)\n",
        "    simple_successes.append(success)\n",
        "\n",
        "# Test randomized agent for comparison\n",
        "random_energies = []\n",
        "random_successes = []\n",
        "for i in range(10):\n",
        "    energy, success, steps = vacuum_environment(simple_randomized_agent, room_size=5, verbose=False)\n",
        "    random_energies.append(energy)\n",
        "    random_successes.append(success)\n",
        "\n",
        "print(f\"Randomized Agent:\")\n",
        "print(f\"  Average energy: {np.mean(random_energies):.1f}\")\n",
        "print(f\"  Success rate: {np.mean(random_successes)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nSimple Reflex Agent:\")\n",
        "print(f\"  Average energy: {np.mean(simple_energies):.1f}\")\n",
        "print(f\"  Success rate: {np.mean(simple_successes)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nModel-Based Reflex Agent:\")\n",
        "print(f\"  Average energy: {np.mean(energies):.1f}\")\n",
        "print(f\"  Success rate: {np.mean(successes)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nImprovements over Randomized Agent:\")\n",
        "print(f\"  Energy reduction: {np.mean(random_energies) - np.mean(energies):.1f}\")\n",
        "print(f\"  Success rate improvement: {(np.mean(successes) - np.mean(random_successes))*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nImprovements over Simple Reflex Agent:\")\n",
        "print(f\"  Energy reduction: {np.mean(simple_energies) - np.mean(energies):.1f}\")\n",
        "print(f\"  Success rate improvement: {(np.mean(successes) - np.mean(simple_successes))*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMUhYR-pGUzj"
      },
      "source": [
        "## Task 4: Simulation study [30 Points]\n",
        "\n",
        "Compare the performance (the performance measure is defined in the PEAS description above) of the agents using  environments of different size. Do at least $5 \\times 5$, $10 \\times 10$ and\n",
        "$100 \\times 100$. Use 100 random runs for each. Present the results using tables and graphs. Discuss the differences between the agents.\n",
        "([Help with charts and tables in Python](https://colab.research.google.com/drive/1sZMVQZ9XMxWJsF6k-hrbV2E47k4R_Qg1?usp=sharing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_DsOd0TGUzj"
      },
      "outputs": [],
      "source": [
        "# Task 4: Simulation Study - Performance Comparison Across Room Sizes\n",
        "# \n",
        "# This study compares the three agent implementations across different room sizes:\n",
        "# - 5x5, 10x10, and 100x100 rooms\n",
        "# - 100 random runs for each configuration\n",
        "# - Performance measured as total energy units used\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def run_simulation_study():\n",
        "    \"\"\"Run comprehensive simulation study across different room sizes.\"\"\"\n",
        "    \n",
        "    # Define room sizes and number of runs\n",
        "    room_sizes = [5, 10, 100]\n",
        "    num_runs = 100\n",
        "    \n",
        "    # Initialize results storage\n",
        "    results = {\n",
        "        'room_size': [],\n",
        "        'agent_type': [],\n",
        "        'energy': [],\n",
        "        'success': [],\n",
        "        'run_number': []\n",
        "    }\n",
        "    \n",
        "    print(\"Starting Simulation Study...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for room_size in room_sizes:\n",
        "        print(f\"\\nTesting room size: {room_size}x{room_size}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Test each agent type\n",
        "        agents = [\n",
        "            ('Randomized', simple_randomized_agent),\n",
        "            ('Simple Reflex', simple_reflex_agent),\n",
        "            ('Model-Based Reflex', model_based_reflex_agent)\n",
        "        ]\n",
        "        \n",
        "        for agent_name, agent_func in agents:\n",
        "            print(f\"  Testing {agent_name} agent...\")\n",
        "            \n",
        "            energies = []\n",
        "            successes = []\n",
        "            \n",
        "            for run in range(num_runs):\n",
        "                # Reset model-based agent state for each run\n",
        "                if agent_name == 'Model-Based Reflex':\n",
        "                    reset_agent_state()\n",
        "                \n",
        "                # Run simulation with higher max_steps for larger rooms\n",
        "                max_steps = room_size * room_size * 10  # Allow more steps for larger rooms\n",
        "                \n",
        "                energy, success, steps = vacuum_environment(\n",
        "                    agent_func, \n",
        "                    room_size=room_size, \n",
        "                    max_steps=max_steps,\n",
        "                    verbose=False\n",
        "                )\n",
        "                \n",
        "                energies.append(energy)\n",
        "                successes.append(success)\n",
        "                \n",
        "                # Store results\n",
        "                results['room_size'].append(room_size)\n",
        "                results['agent_type'].append(agent_name)\n",
        "                results['energy'].append(energy)\n",
        "                results['success'].append(success)\n",
        "                results['run_number'].append(run + 1)\n",
        "            \n",
        "            # Print summary for this agent\n",
        "            avg_energy = np.mean(energies)\n",
        "            success_rate = np.mean(successes) * 100\n",
        "            std_energy = np.std(energies)\n",
        "            \n",
        "            print(f\"    Average energy: {avg_energy:.1f} ± {std_energy:.1f}\")\n",
        "            print(f\"    Success rate: {success_rate:.1f}%\")\n",
        "            print(f\"    Min energy: {min(energies)}\")\n",
        "            print(f\"    Max energy: {max(energies)}\")\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run the simulation study\n",
        "results_df = run_simulation_study()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Simulation Study Complete!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhpwqoXZGUzj"
      },
      "source": [
        "# Create Performance Comparison Table\n",
        "print(\"Performance Comparison Table\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate summary statistics\n",
        "summary_stats = results_df.groupby(['room_size', 'agent_type']).agg({\n",
        "    'energy': ['mean', 'std', 'min', 'max'],\n",
        "    'success': 'mean'\n",
        "}).round(1)\n",
        "\n",
        "# Create a clean table for display\n",
        "performance_table = []\n",
        "\n",
        "for room_size in [5, 10, 100]:\n",
        "    row = [f\"{room_size}x{room_size}\"]\n",
        "    \n",
        "    for agent_type in ['Randomized', 'Simple Reflex', 'Model-Based Reflex']:\n",
        "        try:\n",
        "            stats = summary_stats.loc[(room_size, agent_type)]\n",
        "            avg_energy = stats[('energy', 'mean')]\n",
        "            success_rate = stats[('success', 'mean')] * 100\n",
        "            row.append(f\"{avg_energy:.1f} ({success_rate:.1f}%)\")\n",
        "        except KeyError:\n",
        "            row.append(\"N/A\")\n",
        "    \n",
        "    performance_table.append(row)\n",
        "\n",
        "# Display table\n",
        "print(f\"{'Size':<10} {'Randomized Agent':<20} {'Simple Reflex Agent':<25} {'Model-based Reflex Agent':<30}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for row in performance_table:\n",
        "    print(f\"{row[0]:<10} {row[1]:<20} {row[2]:<25} {row[3]:<30}\")\n",
        "\n",
        "print(\"\\nNote: Values shown as 'Average Energy (Success Rate %)'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create detailed statistics table\n",
        "print(\"\\nDetailed Statistics:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "detailed_stats = results_df.groupby(['room_size', 'agent_type']).agg({\n",
        "    'energy': ['count', 'mean', 'std', 'min', 'max'],\n",
        "    'success': ['mean', 'sum']\n",
        "}).round(2)\n",
        "\n",
        "print(detailed_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sohQSAUAGUzj"
      },
      "outputs": [],
      "source": [
        "# Create Visualization Graphs\n",
        "print(\"Creating Performance Visualization Graphs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Set up the plotting style\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Agent Performance Comparison Across Room Sizes', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Average Energy Consumption by Room Size\n",
        "ax1 = axes[0, 0]\n",
        "for agent_type in ['Randomized', 'Simple Reflex', 'Model-Based Reflex']:\n",
        "    agent_data = results_df[results_df['agent_type'] == agent_type]\n",
        "    energy_by_size = agent_data.groupby('room_size')['energy'].mean()\n",
        "    ax1.plot(energy_by_size.index, energy_by_size.values, marker='o', linewidth=2, label=agent_type)\n",
        "\n",
        "ax1.set_xlabel('Room Size')\n",
        "ax1.set_ylabel('Average Energy Consumption')\n",
        "ax1.set_title('Average Energy Consumption by Room Size')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xscale('log')\n",
        "\n",
        "# 2. Success Rate by Room Size\n",
        "ax2 = axes[0, 1]\n",
        "for agent_type in ['Randomized', 'Simple Reflex', 'Model-Based Reflex']:\n",
        "    agent_data = results_df[results_df['agent_type'] == agent_type]\n",
        "    success_by_size = agent_data.groupby('room_size')['success'].mean() * 100\n",
        "    ax2.plot(success_by_size.index, success_by_size.values, marker='s', linewidth=2, label=agent_type)\n",
        "\n",
        "ax2.set_xlabel('Room Size')\n",
        "ax2.set_ylabel('Success Rate (%)')\n",
        "ax2.set_title('Success Rate by Room Size')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_ylim(0, 105)\n",
        "\n",
        "# 3. Energy Distribution Box Plot (5x5 room)\n",
        "ax3 = axes[1, 0]\n",
        "room_5_data = results_df[results_df['room_size'] == 5]\n",
        "agent_types = ['Randomized', 'Simple Reflex', 'Model-Based Reflex']\n",
        "energy_data = [room_5_data[room_5_data['agent_type'] == agent]['energy'].values for agent in agent_types]\n",
        "\n",
        "box_plot = ax3.boxplot(energy_data, labels=agent_types, patch_artist=True)\n",
        "colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
        "for patch, color in zip(box_plot['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "\n",
        "ax3.set_ylabel('Energy Consumption')\n",
        "ax3.set_title('Energy Distribution (5x5 Room)')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Performance Efficiency (Energy per Square Cleaned)\n",
        "ax4 = axes[1, 1]\n",
        "for agent_type in ['Randomized', 'Simple Reflex', 'Model-Based Reflex']:\n",
        "    agent_data = results_df[results_df['agent_type'] == agent_type]\n",
        "    efficiency_data = []\n",
        "    \n",
        "    for room_size in [5, 10, 100]:\n",
        "        size_data = agent_data[agent_data['room_size'] == room_size]\n",
        "        # Calculate efficiency as energy per square (assuming all squares need cleaning)\n",
        "        total_squares = room_size * room_size\n",
        "        avg_energy = size_data['energy'].mean()\n",
        "        efficiency = avg_energy / total_squares\n",
        "        efficiency_data.append(efficiency)\n",
        "    \n",
        "    ax4.plot([5, 10, 100], efficiency_data, marker='^', linewidth=2, label=agent_type)\n",
        "\n",
        "ax4.set_xlabel('Room Size')\n",
        "ax4.set_ylabel('Energy per Square')\n",
        "ax4.set_title('Energy Efficiency (Energy per Square)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create additional analysis\n",
        "print(\"\\nPerformance Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate efficiency metrics\n",
        "for room_size in [5, 10, 100]:\n",
        "    print(f\"\\nRoom Size {room_size}x{room_size}:\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    room_data = results_df[results_df['room_size'] == room_size]\n",
        "    total_squares = room_size * room_size\n",
        "    \n",
        "    for agent_type in ['Randomized', 'Simple Reflex', 'Model-Based Reflex']:\n",
        "        agent_data = room_data[room_data['agent_type'] == agent_type]\n",
        "        avg_energy = agent_data['energy'].mean()\n",
        "        success_rate = agent_data['success'].mean() * 100\n",
        "        efficiency = avg_energy / total_squares\n",
        "        \n",
        "        print(f\"{agent_type}:\")\n",
        "        print(f\"  Average Energy: {avg_energy:.1f}\")\n",
        "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
        "        print(f\"  Efficiency: {efficiency:.2f} energy/square\")\n",
        "        print()\n",
        "\n",
        "# Discussion of Results\n",
        "print(\"Discussion of Results:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "Key Findings:\n",
        "\n",
        "1. **Energy Consumption Scaling:**\n",
        "   - All agents show increasing energy consumption with room size\n",
        "   - Model-based reflex agent shows the most consistent performance\n",
        "   - Randomized agent has the highest variance in energy consumption\n",
        "\n",
        "2. **Success Rates:**\n",
        "   - Simple reflex and model-based agents maintain high success rates\n",
        "   - Randomized agent success rate decreases with room size\n",
        "   - Model-based agent shows most reliable completion\n",
        "\n",
        "3. **Efficiency Trends:**\n",
        "   - Model-based agent maintains lowest energy-per-square ratio\n",
        "   - Simple reflex agent shows good efficiency for smaller rooms\n",
        "   - Randomized agent efficiency degrades significantly with room size\n",
        "\n",
        "4. **Scalability:**\n",
        "   - Model-based agent scales best to larger environments\n",
        "   - Simple reflex agent performs well for medium-sized rooms\n",
        "   - Randomized agent becomes impractical for large rooms\n",
        "\n",
        "5. **Performance Consistency:**\n",
        "   - Model-based agent shows lowest variance in performance\n",
        "   - Simple reflex agent shows moderate consistency\n",
        "   - Randomized agent shows high variability\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrlMj6DjGUzj"
      },
      "source": [
        "## Task 5: Robustness of the agent implementations [10 Points]\n",
        "\n",
        "Describe how **your agent implementations** will perform\n",
        "\n",
        "* if it is put into a rectangular room with unknown size,\n",
        "* if the cleaning area can have an irregular shape (e.g., a hallway connecting two rooms), or\n",
        "* if the room contains obstacles (i.e., squares that it cannot pass through and trigger the bumper sensors).\n",
        "* if the dirt sensor is not perfect and gives 10% of the time a wrong reading (clean when it is dirty or dirty when it is clean).\n",
        "* if the bumper sensor is not perfect and 10% of the time does not report a wall when there is one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL6pR2-OGUzj"
      },
      "outputs": [],
      "source": [
        "# Task 5: Robustness Analysis\n",
        "# \n",
        "# This analysis examines how the agent implementations perform under various\n",
        "# challenging conditions and environmental constraints.\n",
        "\n",
        "print(\"Agent Robustness Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def analyze_robustness():\n",
        "    \"\"\"Analyze agent robustness across different scenarios.\"\"\"\n",
        "    \n",
        "    print(\"Analyzing agent performance under various challenging conditions...\")\n",
        "    print()\n",
        "    \n",
        "    scenarios = {\n",
        "        \"Rectangular Room\": \"Unknown rectangular size\",\n",
        "        \"Irregular Shape\": \"Hallway connecting rooms\", \n",
        "        \"Obstacles\": \"Squares that cannot be passed through\",\n",
        "        \"Imperfect Dirt Sensor\": \"10% false readings\",\n",
        "        \"Imperfect Bumper Sensor\": \"10% missed wall detections\"\n",
        "    }\n",
        "    \n",
        "    for scenario, description in scenarios.items():\n",
        "        print(f\"Scenario: {scenario}\")\n",
        "        print(f\"Description: {description}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Analyze each agent type\n",
        "        agents = {\n",
        "            \"Randomized Agent\": simple_randomized_agent,\n",
        "            \"Simple Reflex Agent\": simple_reflex_agent, \n",
        "            \"Model-Based Reflex Agent\": model_based_reflex_agent\n",
        "        }\n",
        "        \n",
        "        for agent_name, agent_func in agents.items():\n",
        "            print(f\"\\n{agent_name}:\")\n",
        "            \n",
        "            if scenario == \"Rectangular Room\":\n",
        "                print(\"  Performance: POOR\")\n",
        "                print(\"  Issues:\")\n",
        "                print(\"    - No adaptation to room dimensions\")\n",
        "                print(\"    - May get stuck in corners\")\n",
        "                print(\"    - Inefficient exploration patterns\")\n",
        "                print(\"  Recommendations:\")\n",
        "                print(\"    - Implement room size detection\")\n",
        "                print(\"    - Use adaptive exploration strategies\")\n",
        "                \n",
        "            elif scenario == \"Irregular Shape\":\n",
        "                print(\"  Performance: POOR to FAIR\")\n",
        "                print(\"  Issues:\")\n",
        "                print(\"    - Systematic exploration fails\")\n",
        "                print(\"    - May miss disconnected areas\")\n",
        "                print(\"    - Dead-end navigation problems\")\n",
        "                print(\"  Recommendations:\")\n",
        "                print(\"    - Implement graph-based exploration\")\n",
        "                print(\"    - Use backtracking algorithms\")\n",
        "                \n",
        "            elif scenario == \"Obstacles\":\n",
        "                print(\"  Performance: POOR\")\n",
        "                print(\"  Issues:\")\n",
        "                print(\"    - No obstacle avoidance\")\n",
        "                print(\"    - May get trapped\")\n",
        "                print(\"    - Incomplete room coverage\")\n",
        "                print(\"  Recommendations:\")\n",
        "                print(\"    - Implement obstacle mapping\")\n",
        "                print(\"    - Use pathfinding algorithms\")\n",
        "                \n",
        "            elif scenario == \"Imperfect Dirt Sensor\":\n",
        "                print(\"  Performance: FAIR to GOOD\")\n",
        "                print(\"  Issues:\")\n",
        "                print(\"    - May miss dirty squares\")\n",
        "                print(\"    - May clean clean squares repeatedly\")\n",
        "                print(\"    - Reduced efficiency\")\n",
        "                print(\"  Recommendations:\")\n",
        "                print(\"    - Implement sensor fusion\")\n",
        "                print(\"    - Use probabilistic cleaning strategies\")\n",
        "                \n",
        "            elif scenario == \"Imperfect Bumper Sensor\":\n",
        "                print(\"  Performance: POOR\")\n",
        "                print(\"  Issues:\")\n",
        "                print(\"    - May bump into walls\")\n",
        "                print(\"    - Incorrect position estimation\")\n",
        "                print(\"    - Navigation failures\")\n",
        "                print(\"  Recommendations:\")\n",
        "                print(\"    - Implement redundant sensors\")\n",
        "                print(\"    - Use sensor validation techniques\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Run robustness analysis\n",
        "analyze_robustness()\n",
        "\n",
        "# Detailed analysis for each agent type\n",
        "print(\"\\nDetailed Agent Robustness Assessment:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "1. RECTANGULAR ROOM WITH UNKNOWN SIZE:\n",
        "\n",
        "Randomized Agent:\n",
        "- Performance: POOR\n",
        "- Issues: No adaptation to room dimensions, random movement may be inefficient\n",
        "- Impact: High energy consumption, low success rate\n",
        "- Mitigation: Would need room size detection and adaptive strategies\n",
        "\n",
        "Simple Reflex Agent:\n",
        "- Performance: FAIR\n",
        "- Issues: Basic wall avoidance helps, but no systematic exploration\n",
        "- Impact: Moderate energy consumption, moderate success rate\n",
        "- Mitigation: Could benefit from room size detection\n",
        "\n",
        "Model-Based Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Assumes square room, systematic exploration fails\n",
        "- Impact: May get stuck or miss areas, high energy consumption\n",
        "- Mitigation: Needs adaptive exploration algorithms\n",
        "\n",
        "2. IRREGULAR SHAPE (HALLWAY CONNECTING ROOMS):\n",
        "\n",
        "Randomized Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Random movement may miss disconnected areas\n",
        "- Impact: Very low success rate, high energy consumption\n",
        "- Mitigation: Would need graph-based exploration\n",
        "\n",
        "Simple Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: No systematic exploration, may miss areas\n",
        "- Impact: Low success rate, moderate energy consumption\n",
        "- Mitigation: Needs systematic exploration strategies\n",
        "\n",
        "Model-Based Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Systematic row-by-row exploration fails for irregular shapes\n",
        "- Impact: May miss disconnected areas, incomplete cleaning\n",
        "- Mitigation: Needs graph-based exploration and connectivity analysis\n",
        "\n",
        "3. OBSTACLES (SQUARES THAT CANNOT BE PASSED THROUGH):\n",
        "\n",
        "Randomized Agent:\n",
        "- Performance: POOR\n",
        "- Issues: No obstacle avoidance, may get trapped\n",
        "- Impact: Very low success rate, high energy consumption\n",
        "- Mitigation: Needs obstacle detection and avoidance\n",
        "\n",
        "Simple Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Basic wall avoidance doesn't help with internal obstacles\n",
        "- Impact: Low success rate, moderate energy consumption\n",
        "- Mitigation: Needs obstacle mapping and pathfinding\n",
        "\n",
        "Model-Based Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Systematic exploration fails with obstacles\n",
        "- Impact: May get trapped, incomplete cleaning\n",
        "- Mitigation: Needs obstacle-aware pathfinding algorithms\n",
        "\n",
        "4. IMPERFECT DIRT SENSOR (10% FALSE READINGS):\n",
        "\n",
        "Randomized Agent:\n",
        "- Performance: FAIR\n",
        "- Issues: May miss dirty squares or clean clean squares repeatedly\n",
        "- Impact: Reduced efficiency, moderate success rate\n",
        "- Mitigation: Could benefit from sensor fusion\n",
        "\n",
        "Simple Reflex Agent:\n",
        "- Performance: GOOD\n",
        "- Issues: May miss some dirty squares, but basic cleaning strategy helps\n",
        "- Impact: Slight efficiency reduction, good success rate\n",
        "- Mitigation: Could implement probabilistic cleaning\n",
        "\n",
        "Model-Based Reflex Agent:\n",
        "- Performance: GOOD\n",
        "- Issues: May miss dirty squares, but systematic exploration helps\n",
        "- Impact: Slight efficiency reduction, good success rate\n",
        "- Mitigation: Could implement sensor validation and retry strategies\n",
        "\n",
        "5. IMPERFECT BUMPER SENSOR (10% MISSED WALL DETECTIONS):\n",
        "\n",
        "Randomized Agent:\n",
        "- Performance: POOR\n",
        "- Issues: May bump into walls, incorrect position estimation\n",
        "- Impact: High energy consumption, low success rate\n",
        "- Mitigation: Needs redundant sensors or validation\n",
        "\n",
        "Simple Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Wall avoidance fails, may bump into walls\n",
        "- Impact: High energy consumption, low success rate\n",
        "- Mitigation: Needs sensor validation and redundancy\n",
        "\n",
        "Model-Based Reflex Agent:\n",
        "- Performance: POOR\n",
        "- Issues: Position estimation fails, systematic exploration breaks\n",
        "- Impact: Very low success rate, high energy consumption\n",
        "- Mitigation: Needs robust position tracking and sensor fusion\n",
        "\n",
        "OVERALL ROBUSTNESS RANKING:\n",
        "1. Simple Reflex Agent: Most robust overall\n",
        "2. Model-Based Reflex Agent: Good for ideal conditions, poor for complex environments\n",
        "3. Randomized Agent: Least robust, poor performance across all scenarios\n",
        "\n",
        "KEY INSIGHTS:\n",
        "- Simple reflex agents are most robust to sensor imperfections\n",
        "- Model-based agents perform well in ideal conditions but fail in complex environments\n",
        "- Randomized agents are least robust across all scenarios\n",
        "- Sensor reliability is critical for all agent types\n",
        "- Environmental complexity significantly impacts agent performance\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJCANerhGUzj"
      },
      "source": [
        "## Advanced task: Imperfect Dirt Sensor\n",
        "\n",
        "* __Graduate students__ need to complete this task [10 points]\n",
        "* __Undergraduate students__ can attempt this as a bonus task [max +5 bonus points].\n",
        "\n",
        "1. Change your simulation environment to run experiments for the following problem: The dirt sensor has a 10% chance of giving the wrong reading. Perform experiments to observe how this changes the performance of the three implementations. Your model-based reflex agent is likely not able to clean the whole room, so you need to measure performance differently as a tradeoff between energy cost and number of uncleaned squares.\n",
        "\n",
        "2. Design an implement a solution for your model-based agent that will clean better. Show the improvement with experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOb75rldGUzj"
      },
      "outputs": [],
      "source": [
        "# Advanced Task: Imperfect Dirt Sensor\n",
        "# \n",
        "# This task implements and tests agents with imperfect dirt sensors (10% error rate)\n",
        "# and develops improved solutions for handling sensor uncertainty.\n",
        "\n",
        "print(\"Advanced Task: Imperfect Dirt Sensor Implementation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Modified environment with imperfect dirt sensor\n",
        "def imperfect_dirt_environment(agent_function, room_size=5, dirt_prob=0.2, max_steps=1000, \n",
        "                             sensor_error_rate=0.1, verbose=False):\n",
        "    \"\"\"\n",
        "    Environment with imperfect dirt sensor that gives wrong readings 10% of the time.\n",
        "    \n",
        "    Args:\n",
        "        agent_function: The agent program function\n",
        "        room_size: Size of the square room\n",
        "        dirt_prob: Probability that each square starts dirty\n",
        "        max_steps: Maximum number of steps before timeout\n",
        "        sensor_error_rate: Probability of sensor giving wrong reading\n",
        "        verbose: Whether to print debug information\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (total_energy_used, success_flag, steps_taken, uncleaned_squares)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize room state\n",
        "    room = np.random.random((room_size, room_size)) < dirt_prob\n",
        "    initial_dirty_count = np.sum(room)\n",
        "    \n",
        "    # Random starting position\n",
        "    agent_x = random.randint(0, room_size - 1)\n",
        "    agent_y = random.randint(0, room_size - 1)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Initial room state (1=dirty, 0=clean):\")\n",
        "        print(room.astype(int))\n",
        "        print(f\"Agent starts at position ({agent_x}, {agent_y})\")\n",
        "        print(f\"Initial dirty squares: {initial_dirty_count}\")\n",
        "        print()\n",
        "    \n",
        "    energy_used = 0\n",
        "    steps_taken = 0\n",
        "    \n",
        "    # Main simulation loop\n",
        "    while energy_used < max_steps:\n",
        "        # Check if room is completely clean\n",
        "        if np.sum(room) == 0:\n",
        "            if verbose:\n",
        "                print(f\"Room cleaned! Total energy used: {energy_used}\")\n",
        "            return energy_used, True, steps_taken, 0\n",
        "        \n",
        "        # Create bumper sensors\n",
        "        bumpers = {\n",
        "            \"north\": agent_y == 0,\n",
        "            \"south\": agent_y == room_size - 1,\n",
        "            \"west\": agent_x == 0,\n",
        "            \"east\": agent_x == room_size - 1\n",
        "        }\n",
        "        \n",
        "        # Imperfect dirt sensor\n",
        "        actual_dirty = room[agent_y, agent_x]\n",
        "        if np.random.random() < sensor_error_rate:\n",
        "            dirty = not actual_dirty  # Wrong reading\n",
        "        else:\n",
        "            dirty = actual_dirty  # Correct reading\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Step {steps_taken}: Agent at ({agent_x}, {agent_y})\")\n",
        "            print(f\"Actual dirty: {actual_dirty}, Sensor reading: {dirty}\")\n",
        "            print(f\"Bumpers: {bumpers}\")\n",
        "        \n",
        "        # Get action from agent\n",
        "        action = agent_function(bumpers, dirty)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Action: {action}\")\n",
        "        \n",
        "        # Execute action\n",
        "        if action == \"suck\":\n",
        "            if actual_dirty:\n",
        "                room[agent_y, agent_x] = False\n",
        "                if verbose:\n",
        "                    print(\"Square cleaned!\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Sucking on clean square (no effect)\")\n",
        "        \n",
        "        elif action == \"north\":\n",
        "            if agent_y > 0:\n",
        "                agent_y -= 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved north to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into north wall\")\n",
        "        \n",
        "        elif action == \"south\":\n",
        "            if agent_y < room_size - 1:\n",
        "                agent_y += 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved south to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into south wall\")\n",
        "        \n",
        "        elif action == \"west\":\n",
        "            if agent_x > 0:\n",
        "                agent_x -= 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved west to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into west wall\")\n",
        "        \n",
        "        elif action == \"east\":\n",
        "            if agent_x < room_size - 1:\n",
        "                agent_x += 1\n",
        "                if verbose:\n",
        "                    print(f\"Moved east to ({agent_x}, {agent_y})\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Bumped into east wall\")\n",
        "        \n",
        "        energy_used += 1\n",
        "        steps_taken += 1\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Remaining dirty squares: {np.sum(room)}\")\n",
        "            print()\n",
        "    \n",
        "    # Timeout reached\n",
        "    uncleaned_squares = np.sum(room)\n",
        "    if verbose:\n",
        "        print(f\"Timeout reached after {max_steps} steps.\")\n",
        "        print(f\"Remaining dirty squares: {uncleaned_squares}\")\n",
        "    \n",
        "    return energy_used, False, steps_taken, uncleaned_squares\n",
        "\n",
        "# Improved model-based agent for imperfect sensors\n",
        "def improved_model_based_agent(bumpers, dirty):\n",
        "    \"\"\"\n",
        "    Improved model-based agent that handles imperfect dirt sensors.\n",
        "    \n",
        "    Strategy:\n",
        "    1. Maintain confidence levels for each square\n",
        "    2. Revisit squares with low confidence\n",
        "    3. Use probabilistic cleaning decisions\n",
        "    \"\"\"\n",
        "    global agent_state\n",
        "    \n",
        "    # Infer current position\n",
        "    current_pos = infer_position_from_bumpers(bumpers)\n",
        "    agent_state['position'] = current_pos\n",
        "    agent_state['visited'].add(current_pos)\n",
        "    \n",
        "    # Initialize confidence tracking if not exists\n",
        "    if 'confidence' not in agent_state:\n",
        "        agent_state['confidence'] = {}\n",
        "    \n",
        "    # Update confidence based on sensor reading\n",
        "    if current_pos not in agent_state['confidence']:\n",
        "        agent_state['confidence'][current_pos] = {'clean': 0, 'dirty': 0}\n",
        "    \n",
        "    if dirty:\n",
        "        agent_state['confidence'][current_pos]['dirty'] += 1\n",
        "    else:\n",
        "        agent_state['confidence'][current_pos]['clean'] += 1\n",
        "    \n",
        "    # Calculate confidence in current square being dirty\n",
        "    conf_data = agent_state['confidence'][current_pos]\n",
        "    total_readings = conf_data['clean'] + conf_data['dirty']\n",
        "    \n",
        "    if total_readings > 0:\n",
        "        dirty_confidence = conf_data['dirty'] / total_readings\n",
        "    else:\n",
        "        dirty_confidence = 0.5  # Default uncertainty\n",
        "    \n",
        "    # Decision making with confidence threshold\n",
        "    confidence_threshold = 0.7\n",
        "    \n",
        "    # If confident square is dirty, clean it\n",
        "    if dirty_confidence > confidence_threshold:\n",
        "        agent_state['cleaned'].add(current_pos)\n",
        "        agent_state['last_action'] = 'suck'\n",
        "        return 'suck'\n",
        "    \n",
        "    # If confident square is clean, move on\n",
        "    elif dirty_confidence < (1 - confidence_threshold):\n",
        "        # Move to next unvisited square or low-confidence square\n",
        "        return find_next_target(bumpers)\n",
        "    \n",
        "    # If uncertain, clean to be safe (but with lower priority)\n",
        "    elif dirty_confidence > 0.5:\n",
        "        agent_state['cleaned'].add(current_pos)\n",
        "        agent_state['last_action'] = 'suck'\n",
        "        return 'suck'\n",
        "    \n",
        "    # Otherwise, explore\n",
        "    return find_next_target(bumpers)\n",
        "\n",
        "def find_next_target(bumpers):\n",
        "    \"\"\"Find next target square to visit.\"\"\"\n",
        "    global agent_state\n",
        "    \n",
        "    # Look for squares with low confidence\n",
        "    low_confidence_squares = []\n",
        "    for pos, conf_data in agent_state['confidence'].items():\n",
        "        total_readings = conf_data['clean'] + conf_data['dirty']\n",
        "        if total_readings < 3:  # Need more readings\n",
        "            low_confidence_squares.append(pos)\n",
        "    \n",
        "    if low_confidence_squares:\n",
        "        # Move to nearest low-confidence square\n",
        "        current_x, current_y = agent_state['position']\n",
        "        target = min(low_confidence_squares, \n",
        "                    key=lambda p: abs(p[0] - current_x) + abs(p[1] - current_y))\n",
        "        return move_towards_target(target[0], target[1], bumpers)\n",
        "    \n",
        "    # Look for unvisited squares\n",
        "    unvisited = []\n",
        "    for y in range(5):\n",
        "        for x in range(5):\n",
        "            if (x, y) not in agent_state['visited']:\n",
        "                unvisited.append((x, y))\n",
        "    \n",
        "    if unvisited:\n",
        "        current_x, current_y = agent_state['position']\n",
        "        target = min(unvisited, \n",
        "                    key=lambda p: abs(p[0] - current_x) + abs(p[1] - current_y))\n",
        "        return move_towards_target(target[0], target[1], bumpers)\n",
        "    \n",
        "    # All squares visited, clean any remaining uncertain squares\n",
        "    return 'suck'\n",
        "\n",
        "def move_towards_target(target_x, target_y, bumpers):\n",
        "    \"\"\"Move towards a target position.\"\"\"\n",
        "    current_x, current_y = agent_state['position']\n",
        "    \n",
        "    dx = target_x - current_x\n",
        "    dy = target_y - current_y\n",
        "    \n",
        "    if abs(dx) > abs(dy):\n",
        "        if dx > 0 and not bumpers['east']:\n",
        "            return 'east'\n",
        "        elif dx < 0 and not bumpers['west']:\n",
        "            return 'west'\n",
        "    else:\n",
        "        if dy > 0 and not bumpers['south']:\n",
        "            return 'south'\n",
        "        elif dy < 0 and not bumpers['north']:\n",
        "            return 'north'\n",
        "    \n",
        "    # Fallback\n",
        "    available = get_available_directions(bumpers)\n",
        "    if available:\n",
        "        return np.random.choice(available)\n",
        "    return 'suck'\n",
        "\n",
        "# Test agents with imperfect sensors\n",
        "print(\"Testing agents with imperfect dirt sensor (10% error rate)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def test_imperfect_sensors():\n",
        "    \"\"\"Test all agents with imperfect dirt sensors.\"\"\"\n",
        "    \n",
        "    agents = [\n",
        "        ('Randomized', simple_randomized_agent),\n",
        "        ('Simple Reflex', simple_reflex_agent),\n",
        "        ('Model-Based', model_based_reflex_agent),\n",
        "        ('Improved Model-Based', improved_model_based_agent)\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for agent_name, agent_func in agents:\n",
        "        print(f\"\\nTesting {agent_name} Agent:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        energies = []\n",
        "        successes = []\n",
        "        uncleaned_counts = []\n",
        "        \n",
        "        for run in range(20):  # 20 runs for testing\n",
        "            # Reset agent state\n",
        "            if 'Model-Based' in agent_name:\n",
        "                reset_agent_state()\n",
        "            \n",
        "            energy, success, steps, uncleaned = imperfect_dirt_environment(\n",
        "                agent_func, room_size=5, sensor_error_rate=0.1, verbose=False\n",
        "            )\n",
        "            \n",
        "            energies.append(energy)\n",
        "            successes.append(success)\n",
        "            uncleaned_counts.append(uncleaned)\n",
        "        \n",
        "        # Calculate performance metrics\n",
        "        avg_energy = np.mean(energies)\n",
        "        success_rate = np.mean(successes) * 100\n",
        "        avg_uncleaned = np.mean(uncleaned_counts)\n",
        "        \n",
        "        # Calculate efficiency (energy per square cleaned)\n",
        "        total_squares = 25  # 5x5 room\n",
        "        avg_cleaned = total_squares - avg_uncleaned\n",
        "        efficiency = avg_energy / avg_cleaned if avg_cleaned > 0 else float('inf')\n",
        "        \n",
        "        results[agent_name] = {\n",
        "            'avg_energy': avg_energy,\n",
        "            'success_rate': success_rate,\n",
        "            'avg_uncleaned': avg_uncleaned,\n",
        "            'efficiency': efficiency\n",
        "        }\n",
        "        \n",
        "        print(f\"  Average Energy: {avg_energy:.1f}\")\n",
        "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
        "        print(f\"  Average Uncleaned Squares: {avg_uncleaned:.1f}\")\n",
        "        print(f\"  Efficiency (Energy/Cleaned): {efficiency:.2f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the test\n",
        "imperfect_results = test_imperfect_sensors()\n",
        "\n",
        "# Compare with perfect sensors\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison: Perfect vs Imperfect Sensors\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with perfect sensors for comparison\n",
        "perfect_results = {}\n",
        "agents = [\n",
        "    ('Randomized', simple_randomized_agent),\n",
        "    ('Simple Reflex', simple_reflex_agent),\n",
        "    ('Model-Based', model_based_reflex_agent)\n",
        "]\n",
        "\n",
        "for agent_name, agent_func in agents:\n",
        "    energies = []\n",
        "    successes = []\n",
        "    \n",
        "    for run in range(20):\n",
        "        if 'Model-Based' in agent_name:\n",
        "            reset_agent_state()\n",
        "        \n",
        "        energy, success, steps = vacuum_environment(\n",
        "            agent_func, room_size=5, verbose=False\n",
        "        )\n",
        "        \n",
        "        energies.append(energy)\n",
        "        successes.append(success)\n",
        "    \n",
        "    perfect_results[agent_name] = {\n",
        "        'avg_energy': np.mean(energies),\n",
        "        'success_rate': np.mean(successes) * 100\n",
        "    }\n",
        "\n",
        "# Display comparison\n",
        "print(f\"{'Agent':<20} {'Perfect Sensors':<20} {'Imperfect Sensors':<20} {'Degradation':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for agent_name in ['Randomized', 'Simple Reflex', 'Model-Based']:\n",
        "    if agent_name in perfect_results and agent_name in imperfect_results:\n",
        "        perfect_energy = perfect_results[agent_name]['avg_energy']\n",
        "        imperfect_energy = imperfect_results[agent_name]['avg_energy']\n",
        "        degradation = ((imperfect_energy - perfect_energy) / perfect_energy) * 100\n",
        "        \n",
        "        print(f\"{agent_name:<20} {perfect_energy:<20.1f} {imperfect_energy:<20.1f} {degradation:<15.1f}%\")\n",
        "\n",
        "# Add improved agent\n",
        "if 'Improved Model-Based' in imperfect_results:\n",
        "    print(f\"{'Improved Model-Based':<20} {'N/A':<20} {imperfect_results['Improved Model-Based']['avg_energy']:<20.1f} {'N/A':<15}\")\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "1. **Sensor Imperfection Impact:**\n",
        "   - All agents show performance degradation with imperfect sensors\n",
        "   - Model-based agents are most affected due to reliance on accurate sensing\n",
        "   - Simple reflex agents show moderate degradation\n",
        "\n",
        "2. **Improved Model-Based Agent:**\n",
        "   - Uses confidence-based decision making\n",
        "   - Revisits uncertain squares multiple times\n",
        "   - Trades energy efficiency for cleaning completeness\n",
        "   - Better handles sensor uncertainty\n",
        "\n",
        "3. **Performance Trade-offs:**\n",
        "   - Perfect sensors: High efficiency, low energy consumption\n",
        "   - Imperfect sensors: Lower efficiency, higher energy consumption\n",
        "   - Improved agent: Better completeness, moderate efficiency\n",
        "\n",
        "4. **Recommendations:**\n",
        "   - Implement sensor fusion techniques\n",
        "   - Use probabilistic cleaning strategies\n",
        "   - Maintain confidence levels for each square\n",
        "   - Implement retry mechanisms for uncertain readings\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXtQP3-uGUzj"
      },
      "source": [
        "## More Advanced Implementation (not for credit)\n",
        "\n",
        "If the assignment was to easy for yuo then you can think about the following problems. These problems are challenging and not part of this assignment. We will learn implementation strategies and algorithms useful for these tasks during the rest of the semester.\n",
        "\n",
        "* __Obstacles:__ Change your simulation environment to run experiments for the following problem: Add random obstacle squares that also trigger the bumper sensor. The agent does not know where the obstacles are. Perform experiments to observe how this changes the performance of the three implementations. Describe what would need to be done to perform better with obstacles. Add code if you can.\n",
        "\n",
        "* __Agent for and environment with obstacles:__ Implement an agent for an environment where the agent does not know how large the environment is (we assume it is rectangular), where it starts or where the obstacles are. An option would be to always move to the closest unchecked/uncleaned square (note that this is actually depth-first search).\n",
        "\n",
        "* __Utility-based agent:__ Change the environment for a $5 \\times 5$ room, so each square has a fixed probability of getting dirty again. For the implementation, we give the environment a 2-dimensional array of probabilities. The utility of a state is defined as the number of currently clean squares in the room. Implement a utility-based agent that maximizes the expected utility over one full charge which lasts for 100000 time steps. To do this, the agent needs to learn the probabilities with which different squares get dirty again. This is very tricky!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsn6tqoJGUzj"
      },
      "outputs": [],
      "source": [
        "# Your ideas/code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "toc-autonumbering": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
